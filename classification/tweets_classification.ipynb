{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets_classification",
      "provenance": [],
      "collapsed_sections": [
        "ySHDqNfqBSJP",
        "AHHcwTymBfUI",
        "qg9dMKdXCAGY",
        "BUErCv_VE8yQ",
        "6U_S7Y4oLNUD",
        "sIOmhqVA_ZHi",
        "6rnb72o5Ldbb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vANUDvImA8l4"
      },
      "source": [
        "# Tweets Classification\n",
        "\n",
        "Will be used a keras deep learning model to classify the tweets between 1 (positive) or 0 (negative).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySHDqNfqBSJP"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hnu_qAYAl5U"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split,  RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.backend import clear_session\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHHcwTymBfUI"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEO9C4KgBkiw"
      },
      "source": [
        "df_nlp_tweets = pd.read_csv(\"/content/drive/MyDrive/DI - Analise Bitcoin - GPU/4. Execução/2. Analise de Sentimento/tweets_increase_nlp.csv\")\n",
        "\n",
        "df_nlp_tweets['Raise'] = df_nlp_tweets['Raise'].astype(int)\n",
        "df_nlp_tweets['Date'] = df_nlp_tweets['Date'].astype('datetime64[ns]')\n",
        "df_nlp_tweets['Data_processing_3'] = df_nlp_tweets['Data_processing_3'].astype(str)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1yBHV7jDAf_"
      },
      "source": [
        "## NLP Tweets dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "FmqSZ0zgCMZJ",
        "outputId": "c96a8ddf-b8f6-40f2-e287-8da0ffce1226"
      },
      "source": [
        "df_nlp_tweets = df_nlp_tweets[['Tweet','Date', 'Raise', 'Data_processing_1', 'Data_processing_2', 'Data_processing_3', 'Data_processing_4','Data_processing_5']]\n",
        "df_nlp_tweets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Date</th>\n",
              "      <th>Raise</th>\n",
              "      <th>Data_processing_1</th>\n",
              "      <th>Data_processing_2</th>\n",
              "      <th>Data_processing_3</th>\n",
              "      <th>Data_processing_4</th>\n",
              "      <th>Data_processing_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reasons why I'm bullish on #Bitcoin:\\n\\n About...</td>\n",
              "      <td>2021-10-18</td>\n",
              "      <td>1</td>\n",
              "      <td>reasons i'm bullish #bitcoin: retake ath bitco...</td>\n",
              "      <td>reasons i m bullish bitcoin retake ath bitcoin...</td>\n",
              "      <td>reasons bullish bitcoin retake ath bitcoin etf...</td>\n",
              "      <td>reason bullish bitcoin retak ath bitcoin etf a...</td>\n",
              "      <td>reason bullish bitcoin retak ath bitcoin etf a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>First U.S. #bitcoin ETF looks set to debut Tue...</td>\n",
              "      <td>2021-10-18</td>\n",
              "      <td>1</td>\n",
              "      <td>first u.s. #bitcoin etf looks set debut tuesda...</td>\n",
              "      <td>first u s bitcoin etf looks set debut tuesday ...</td>\n",
              "      <td>first u bitcoin etf looks set debut tuesday pr...</td>\n",
              "      <td>first u bitcoin etf look set debut tuesday pro...</td>\n",
              "      <td>first u bitcoin etf look set debut tuesday pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Top10 Twitter KOL &amp; Gem Rating Oct.18\\n1 $DOGE...</td>\n",
              "      <td>2021-10-18</td>\n",
              "      <td>1</td>\n",
              "      <td>top10 twitter kol &amp; gem rating oct.18 1 $doge ...</td>\n",
              "      <td>top10 twitter kol gem rating oct 18 1 doge 2 x...</td>\n",
              "      <td>top10 twitter kol gem rating oct 18 1 doge 2 x...</td>\n",
              "      <td>top10 twitter kol gem rate oct 18 1 doge 2 xrp...</td>\n",
              "      <td>top10 twitter kol gem rate oct 18 1 doge david...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#bitcoin over $62,000 again. \\n\\n$3,000 to go.</td>\n",
              "      <td>2021-10-18</td>\n",
              "      <td>1</td>\n",
              "      <td>#bitcoin $62,000 again. $3,000 go.</td>\n",
              "      <td>bitcoin 62 000 again 3 000 go</td>\n",
              "      <td>bitcoin 62 000 3 000 go</td>\n",
              "      <td>bitcoin 62 000 3 000 go</td>\n",
              "      <td>bitcoin 62 000 3 000 go</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Shiba Inu could Reach 50 Cents By December 202...</td>\n",
              "      <td>2021-10-18</td>\n",
              "      <td>1</td>\n",
              "      <td>shiba inu could reach 50 cents december 2022? ...</td>\n",
              "      <td>shiba inu could reach 50 cents december 2022 h...</td>\n",
              "      <td>shiba inu could reach 50 cents december 2022 m...</td>\n",
              "      <td>shiba inu could reach 50 cent decemb 2022 math...</td>\n",
              "      <td>shiba inu could reach 50 cent decemb 2022 math...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63313</th>\n",
              "      <td>\"\\n@IconicExpert\\n: @nomaxpi \\n@CryptoCobain\\n...</td>\n",
              "      <td>2014-09-17</td>\n",
              "      <td>0</td>\n",
              "      <td>\" @iconicexpert : @nomaxpi @cryptocobain im wo...</td>\n",
              "      <td>iconicexpert nomaxpi cryptocobain im worth tim...</td>\n",
              "      <td>iconicexpert nomaxpi cryptocobain im worth tim...</td>\n",
              "      <td>iconicexpert nomaxpi cryptocobain im worth tim...</td>\n",
              "      <td>iconicexpert nomaxpi cryptocobain im worth tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63314</th>\n",
              "      <td>Can someone send me their btc address? Theblon...</td>\n",
              "      <td>2014-09-17</td>\n",
              "      <td>0</td>\n",
              "      <td>someone send btc address? theblonde offered bi...</td>\n",
              "      <td>someone send btc address theblonde offered bit...</td>\n",
              "      <td>someone send btc address theblonde offered bit...</td>\n",
              "      <td>someon send btc address theblond offer bitcoin...</td>\n",
              "      <td>someon send btc address theblond offer bitcoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63315</th>\n",
              "      <td>\"\\n@moolah_io\\n: @GambitBTC $VIA is on the fin...</td>\n",
              "      <td>2014-09-17</td>\n",
              "      <td>0</td>\n",
              "      <td>\" @moolah_io : @gambitbtc $via final list. ple...</td>\n",
              "      <td>moolah_io gambitbtc via final list please chec...</td>\n",
              "      <td>moolah_io gambitbtc via final list please chec...</td>\n",
              "      <td>moolah_io gambitbtc via final list pleas check...</td>\n",
              "      <td>moolah_io gambitbtc via final list pleas check...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63316</th>\n",
              "      <td>What if we all take out loans + buy bitcoin. D...</td>\n",
              "      <td>2014-09-17</td>\n",
              "      <td>0</td>\n",
              "      <td>take loans + buy bitcoin. default loans. cause...</td>\n",
              "      <td>take loans buy bitcoin default loans cause len...</td>\n",
              "      <td>take loans buy bitcoin default loans cause len...</td>\n",
              "      <td>take loan buy bitcoin default loan caus lend c...</td>\n",
              "      <td>take loan buy bitcoin default loan caus lend c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63317</th>\n",
              "      <td>Funny indicator for types of people in crypto:...</td>\n",
              "      <td>2014-09-17</td>\n",
              "      <td>0</td>\n",
              "      <td>funny indicator types people crypto: claim 're...</td>\n",
              "      <td>funny indicator types people claim really succ...</td>\n",
              "      <td>funny indicator types people claim really succ...</td>\n",
              "      <td>funni indic type peopl claim realli success prior</td>\n",
              "      <td>funni indic type peopl crypto claim realli suc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63318 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet  ...                                  Data_processing_5\n",
              "0      Reasons why I'm bullish on #Bitcoin:\\n\\n About...  ...  reason bullish bitcoin retak ath bitcoin etf a...\n",
              "1      First U.S. #bitcoin ETF looks set to debut Tue...  ...  first u bitcoin etf look set debut tuesday pro...\n",
              "2      Top10 Twitter KOL & Gem Rating Oct.18\\n1 $DOGE...  ...  top10 twitter kol gem rate oct 18 1 doge david...\n",
              "3         #bitcoin over $62,000 again. \\n\\n$3,000 to go.  ...                            bitcoin 62 000 3 000 go\n",
              "4      Shiba Inu could Reach 50 Cents By December 202...  ...  shiba inu could reach 50 cent decemb 2022 math...\n",
              "...                                                  ...  ...                                                ...\n",
              "63313  \"\\n@IconicExpert\\n: @nomaxpi \\n@CryptoCobain\\n...  ...  iconicexpert nomaxpi cryptocobain im worth tim...\n",
              "63314  Can someone send me their btc address? Theblon...  ...  someon send btc address theblond offer bitcoin...\n",
              "63315  \"\\n@moolah_io\\n: @GambitBTC $VIA is on the fin...  ...  moolah_io gambitbtc via final list pleas check...\n",
              "63316  What if we all take out loans + buy bitcoin. D...  ...  take loan buy bitcoin default loan caus lend c...\n",
              "63317  Funny indicator for types of people in crypto:...  ...  funni indic type peopl crypto claim realli suc...\n",
              "\n",
              "[63318 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg9dMKdXCAGY"
      },
      "source": [
        "## Create a dataset with only tweets and raise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4VauxS_0CFWf",
        "outputId": "2e781912-cc53-401d-c93e-27b75adf4719"
      },
      "source": [
        "df_tweets_raise = df_nlp_tweets[['Tweet', 'Raise']]\n",
        "df_tweets_raise"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Raise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reasons why I'm bullish on #Bitcoin:\\n\\n About...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>First U.S. #bitcoin ETF looks set to debut Tue...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Top10 Twitter KOL &amp; Gem Rating Oct.18\\n1 $DOGE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#bitcoin over $62,000 again. \\n\\n$3,000 to go.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Shiba Inu could Reach 50 Cents By December 202...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63313</th>\n",
              "      <td>\"\\n@IconicExpert\\n: @nomaxpi \\n@CryptoCobain\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63314</th>\n",
              "      <td>Can someone send me their btc address? Theblon...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63315</th>\n",
              "      <td>\"\\n@moolah_io\\n: @GambitBTC $VIA is on the fin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63316</th>\n",
              "      <td>What if we all take out loans + buy bitcoin. D...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63317</th>\n",
              "      <td>Funny indicator for types of people in crypto:...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63318 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Tweet  Raise\n",
              "0      Reasons why I'm bullish on #Bitcoin:\\n\\n About...      1\n",
              "1      First U.S. #bitcoin ETF looks set to debut Tue...      1\n",
              "2      Top10 Twitter KOL & Gem Rating Oct.18\\n1 $DOGE...      1\n",
              "3         #bitcoin over $62,000 again. \\n\\n$3,000 to go.      1\n",
              "4      Shiba Inu could Reach 50 Cents By December 202...      1\n",
              "...                                                  ...    ...\n",
              "63313  \"\\n@IconicExpert\\n: @nomaxpi \\n@CryptoCobain\\n...      0\n",
              "63314  Can someone send me their btc address? Theblon...      0\n",
              "63315  \"\\n@moolah_io\\n: @GambitBTC $VIA is on the fin...      0\n",
              "63316  What if we all take out loans + buy bitcoin. D...      0\n",
              "63317  Funny indicator for types of people in crypto:...      0\n",
              "\n",
              "[63318 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9mu3BiCE63"
      },
      "source": [
        "Vectorizing the "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUErCv_VE8yQ"
      },
      "source": [
        "# Base Line definition\n",
        "\n",
        "Base line accuracy: 56.475%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq7b3iufGJuF"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPXgd6qkJM0C",
        "outputId": "2b48258b-7c10-411c-aa9e-a30f71083292"
      },
      "source": [
        "df_tweets_raise['Tweet'] = df_tweets_raise['Tweet'].values.astype('U')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ3syQyGFIhW"
      },
      "source": [
        "tweets = df_tweets_raise['Tweet'].values\n",
        "y = df_tweets_raise['Raise'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.25, \n",
        "    random_state=1000\n",
        "    )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4DBexS1G-n-"
      },
      "source": [
        "Creating vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX3TSmE3HBpt",
        "outputId": "5c882571-0056-4f9e-e82f-4c340229c3d2"
      },
      "source": [
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(df_tweets_raise['Tweet'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(lowercase=False, min_df=0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V3Ht9PeKQ0r"
      },
      "source": [
        "Vectorizing the train and the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXiiSC13GRWM",
        "outputId": "fe20f083-8da5-491a-d152-929a939b1a61"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "X_train = vectorizer.transform(x_train)\n",
        "X_test  = vectorizer.transform(x_test)\n",
        "X_train"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<47488x35311 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 800964 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U_S7Y4oLNUD"
      },
      "source": [
        "## Logistic Regression\n",
        "Accuracy: 56.475%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neqGGcP9G8oD",
        "outputId": "e705cc0a-8544-4783-dab4-79e269db79e8"
      },
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0D-kL4nKfwF",
        "outputId": "fb8c6cbf-8425-4a5a-8cec-49ab6f49c37a"
      },
      "source": [
        "print(\"Accuracy: \" + '{:.3f}'.format(score*100) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 56.469%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu524oMxLQ1z"
      },
      "source": [
        "# Keras Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2jzd83zpq1"
      },
      "source": [
        "## Sequencial Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIOmhqVA_ZHi"
      },
      "source": [
        "### Sequencial Model with tweets column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCzRO872Cz0_"
      },
      "source": [
        "tweets = df_tweets_raise['Tweet'].values\n",
        "y = df_tweets_raise['Raise'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.25, \n",
        "    random_state=1000\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilwK8RANC5Sw"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "X_train = vectorizer.transform(x_train)\n",
        "X_test  = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduFZ_AjKpbv"
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POYv6HX-a8W0",
        "outputId": "79dfe4d9-d7b0-4ca9-dd7a-ca4ec68a6893"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 10)                353120    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 353,131\n",
            "Trainable params: 353,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ar2D8T8bI1U",
        "outputId": "0d031fd8-e572-484a-b6ed-263fe32c008b"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0QsgiaPzRRE",
        "outputId": "ee0a1795-71a1-46ac-9a7a-51f170273416"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy*100))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 92.7329\n",
            "Testing Accuracy:  56.1213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnb72o5Ldbb"
      },
      "source": [
        "### Sequencial Model with Data_processing_1 column\n",
        "Testing Accuracy:  56.4940"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xU1swRLmPT"
      },
      "source": [
        "tweets = df_nlp_tweets['Data_processing_1'].astype(str).values\n",
        "y = df_nlp_tweets['Raise'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.25, \n",
        "    random_state=1000\n",
        "    )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3WmNonNLpaa"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "X_train = vectorizer.transform(x_train)\n",
        "X_test  = vectorizer.transform(x_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN_1exXyOm2s"
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g4of5GnOpri",
        "outputId": "eb750db1-8fd8-4147-f359-b75e25ec6f02"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 10)                353100    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 353,111\n",
            "Trainable params: 353,111\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwm42cd1Othy",
        "outputId": "c3cbdcc1-8509-466b-c427-c4aa8624ac71"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_1/dense_2/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1IeNSyAOzbE",
        "outputId": "f92f5994-a0e2-464f-dff0-abccdf821103"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy*100))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 93.4615\n",
            "Testing Accuracy:  56.4940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq3pWZOMHFHU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj3Ibnem5abk"
      },
      "source": [
        "### Sequencial Model with Data_processing_3 column\n",
        "Testing Accuracy:  55.9760"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcOXIZG15abk"
      },
      "source": [
        "tweets = df_nlp_tweets['Data_processing_3'].values\n",
        "y = df_nlp_tweets['Raise'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.25, \n",
        "    random_state=1000\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQt_cBkf5abk"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "X_train = vectorizer.transform(x_train)\n",
        "X_test  = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T22efFG5abl"
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQUbYh5O5abl",
        "outputId": "344b1b26-216e-47e3-9025-ed7390872150"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 10)                351450    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 351,461\n",
            "Trainable params: 351,461\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b3LWqTo5abl",
        "outputId": "7b808ee0-0f99-4c27-df93-f09e0d49404b"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=100,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81OODYDQ5abl",
        "outputId": "a8eb6a76-d432-47bb-957d-3b63018aed05"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy*100))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 92.7161\n",
            "Testing Accuracy:  55.9760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu8xjuBRzs2F"
      },
      "source": [
        "## LabelEncoder and HotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbbicr_Y5VvK"
      },
      "source": [
        "tweets = df_tweets_raise['Tweet'].values\n",
        "y = df_tweets_raise['Raise'].values\n",
        "encoder = LabelEncoder()\n",
        "tweets_label_encoder = encoder.fit_transform(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1rdETws8c2j"
      },
      "source": [
        "## Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgLvwPI50yec"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imyY95Yim4o4"
      },
      "source": [
        "clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTgYeVlk7Rot"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.15, \n",
        "    random_state=1000\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg1E6vl2hZln"
      },
      "source": [
        "- alterar parametros do toknizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT6ePlXO7L90",
        "outputId": "cd28d90d-63c1-49fd-9267-d0672501b555"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(x_train[2])\n",
        "print(X_train[2])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SpaceX makes it cheaper to launch rockets.\n",
            "\n",
            "Someone will make it cheaper to discover medicine, produce construction materials, & feed humans\n",
            "[2103, 276, 12, 2723, 2, 798, 4943, 180, 21, 67, 12, 2723, 2, 1921, 2309, 1663, 787]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfX9oHfp8Rbt"
      },
      "source": [
        "max_len_tweet = 280\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len_tweet)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=max_len_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ns9ArC38gqT"
      },
      "source": [
        "### Sequecial model with Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUizJS_X8nS7",
        "outputId": "354c9043-4744-40e9-f842-0717dc7d9178"
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=max_len_tweet))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 280, 50)           2061850   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 14000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                140010    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,201,871\n",
            "Trainable params: 2,201,871\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJbl3TkW9QL0",
        "outputId": "00cd3509-76fd-47ea-96ef-0384c670c9d7"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5419\n",
            "Testing Accuracy:  0.5396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV-zOLwsKNWG"
      },
      "source": [
        "### Sequecial model with Word Embeddings and GlobalMaxPool1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcXCagXxez_"
      },
      "source": [
        "clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ6j3hHGx0JP"
      },
      "source": [
        "tweets = df_tweets_raise['Tweet'].values\n",
        "y = df_tweets_raise['Raise'].values\n",
        "encoder = LabelEncoder()\n",
        "tweets_label_encoder = encoder.fit_transform(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR0kJFmhxnJZ",
        "outputId": "70db75b0-a4cc-45bd-fa56-6d53793f7920"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.15, \n",
        "    random_state=1000\n",
        "    )\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(x_train[2])\n",
        "print(X_train[2])\n",
        "\n",
        "\n",
        "\n",
        "max_len_tweet = 280\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len_tweet)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=max_len_tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SpaceX makes it cheaper to launch rockets.\n",
            "\n",
            "Someone will make it cheaper to discover medicine, produce construction materials, & feed humans\n",
            "[2103, 276, 12, 2723, 2, 798, 4943, 180, 21, 67, 12, 2723, 2, 1921, 2309, 1663, 787]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyBJ7bqVKPuW",
        "outputId": "d2a7ed0c-fe47-4a35-8b77-3d9958df05f1"
      },
      "source": [
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=max_len_tweet))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 280, 50)           2061850   \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 50)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                510       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,062,371\n",
            "Trainable params: 2,062,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES889372KX82",
        "outputId": "6287a6ed-ec9a-41ff-b5d9-3fe18f806f3d"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9194\n",
            "Testing Accuracy:  0.5665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNsRbfDxx1JX"
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovcr45DBFDTi"
      },
      "source": [
        "### Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJeO70lozxQT"
      },
      "source": [
        "clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6MTQgza4Onc",
        "outputId": "8b92dcbc-da36-455e-cde2-f7ffe84fe2e9"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_len_tweet))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 280, 100)          4123700   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 276, 128)          64128     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,189,129\n",
            "Trainable params: 4,189,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccp0L_aoFlwW"
      },
      "source": [
        "tweets = df_tweets_raise['Tweet'].values\n",
        "y = df_tweets_raise['Raise'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    tweets, \n",
        "    y, \n",
        "    test_size=0.25, \n",
        "    random_state=1000\n",
        "    )\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "max_len_tweet = 280\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len_tweet)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=max_len_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7_jGfn2EFNU1",
        "outputId": "906bc713-f5a0-4e4a-af43-7ba5d13bb640"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9090\n",
            "Testing Accuracy:  0.5545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-cDQcPFhCkX"
      },
      "source": [
        "- Mudar a din para 25, 50, 100\n",
        "\n",
        "- alterar epochs\n",
        "\n",
        "- alterar batch size\n",
        "\n",
        "- alterar test_size"
      ]
    }
  ]
}